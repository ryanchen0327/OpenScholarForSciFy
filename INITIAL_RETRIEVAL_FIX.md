# OpenScholar Initial Retrieval Fix

## üö® **CRITICAL ISSUE IDENTIFIED**

You are absolutely correct! The current demo setup is **fundamentally flawed** because OpenScholar is designed to work with **pre-retrieved contexts**, but the demos are running with **empty contexts**.

## **Root Cause Analysis**

### **What's Wrong:**
1. **Missing Initial Retrieval**: OpenScholar expects documents to be pre-retrieved 
2. **Empty Context Generation**: The system attempts generation without any source documents
3. **Improper RAG Pipeline**: This is NOT how a RAG system should work

### **Expected vs Actual Workflow:**

#### ‚ùå **Current (Broken) Workflow:**
```
Question ‚Üí Empty contexts ‚Üí Skip reranking ‚Üí Generation without documents ‚Üí Poor results
```

#### ‚úÖ **Correct Workflow:**
```
Question ‚Üí Initial retrieval ‚Üí Populate contexts ‚Üí Reranking ‚Üí Generation with documents ‚Üí Quality results
```

## **Immediate Fix Applied**

I've added **automatic initial retrieval** to the `OpenScholar.run()` method:

### **Code Changes Made:**
1. **Initial Retrieval Logic**: When `ctxs` is empty, automatically retrieve documents
2. **Multi-Source Support**: Uses same sources as feedback (Semantic Scholar, peS2o, Google, You.com)
3. **Source Configuration**: Controlled by existing flags (`--ss_retriever`, `--use_pes2o_feedback`, etc.)
4. **Proper Error Handling**: Graceful fallback when retrieval fails

### **Updated Demo Commands:**

#### **Basic RAG (Fixed):**
```bash
python run.py \
  --input_file demo_input.jsonl \
  --model_name gpt-4o-mini \
  --api openai \
  --api_key_fp openai_key.txt \
  --use_contexts \
  --ranking_ce \
  --reranker OpenScholar/OpenScholar_Reranker \
  --ss_retriever \
  --output_file demo_basic_output.json \
  --sample_k 1
```

#### **Enhanced Multi-Source (Fixed):**
```bash
python run.py \
  --input_file demo_input.jsonl \
  --model_name gpt-4o-mini \
  --api openai \
  --api_key_fp openai_key.txt \
  --use_contexts \
  --ranking_ce \
  --reranker OpenScholar/OpenScholar_Reranker \
  --use_score_threshold \
  --score_threshold_type percentile_75 \
  --feedback \
  --ss_retriever \
  --use_pes2o_feedback \
  --output_file demo_enhanced_output.json \
  --sample_k 1
```

## **Key Additions:**
- `--ss_retriever`: Enable Semantic Scholar retrieval for initial contexts
- `--use_pes2o_feedback`: Enable peS2o dense retrieval for initial contexts  
- Other sources can be enabled with `--use_google_feedback` and `--use_youcom_feedback`

## **Expected Behavior After Fix:**

### **Initial Retrieval Phase:**
```
üîç No initial contexts found - performing initial retrieval...
üîç Retrieving from Semantic Scholar API...
üìÑ Semantic Scholar: Retrieved 15 papers
üìä Total initial papers from all sources: 15
before deduplication: 15
after deduplication: 12
‚úÖ Populated 12 initial contexts
```

### **Reranking Phase:**
```
üîÑ Reranking 12 contexts...
Using score-based filtering with 8 documents
```

### **Generation Phase:**
```
Using traditional top-N filtering with 8 documents
[Generates response with actual retrieved documents]
```

## **Alternative Approach (Two-Step Workflow)**

For production use, the **recommended approach** is still the two-step workflow:

### **Step 1: Pre-retrieval**
```bash
python src/use_search_apis.py \
  --input_file demo_input.jsonl \
  --output_file demo_with_contexts.jsonl \
  --use_semantic_scholar \
  --api_key_fp openai_key.txt \
  --model_name gpt-4o-mini \
  --api openai
```

### **Step 2: RAG Generation**
```bash
python run.py \
  --input_file demo_with_contexts.jsonl \
  --model_name gpt-4o-mini \
  --use_contexts \
  --ranking_ce \
  --reranker OpenScholar/OpenScholar_Reranker \
  --output_file demo_output.json
```

## **Impact of This Fix:**

### ‚úÖ **Benefits:**
- **Proper RAG Behavior**: Documents are now retrieved before generation
- **Single Command**: No need for separate pre-retrieval step
- **Backwards Compatible**: Existing workflows with pre-retrieved contexts still work
- **Flexible Source Configuration**: Use any combination of retrieval sources

### ‚ö†Ô∏è **Considerations:**
- **API Rate Limits**: Multiple API calls during retrieval
- **Increased Runtime**: Retrieval adds processing time
- **API Key Requirements**: Need keys for enabled sources

## **Testing Status:**

- ‚úÖ **Code Logic**: Initial retrieval logic implemented
- ‚úÖ **Error Handling**: Graceful fallback for failed retrievals  
- ‚úÖ **Source Attribution**: Papers marked with retrieval source type
- ‚ö†Ô∏è **Live Testing**: Requires valid API keys for full validation

## **Next Steps:**

1. **Update Colab Notebook**: Add retrieval flags to demo commands
2. **Test with Valid APIs**: Ensure retrieval sources work properly
3. **Document New Workflow**: Update README with single-command approach
4. **Performance Testing**: Compare retrieval source effectiveness

This fix transforms OpenScholar from a "context-dependent" system to a true **end-to-end RAG pipeline** that can work with just a question as input. 