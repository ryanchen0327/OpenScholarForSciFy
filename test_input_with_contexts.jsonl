{"input": "What are the main advantages of retrieval-augmented generation over traditional language models?", "ctxs": [{"title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "text": "We explore retrieval-augmented generation (RAG), which uses a pre-trained parametric memory (a seq2seq model) and a non-parametric memory (a dense vector index of Wikipedia) to generate responses. RAG models can generate more specific, diverse and factual responses than seq2seq models. For knowledge-intensive tasks, we achieve state-of-the-art results on three open-domain QA datasets.", "citation_counts": 1500}, {"title": "FiD: Leveraging Passage Retrieval with Generative Models", "text": "Fusion-in-Decoder (FiD) leverages the power of retrieval with the generation capabilities of large language models. By processing multiple retrieved passages independently in the encoder and fusing information in the decoder, FiD achieves strong performance on knowledge-intensive tasks while maintaining computational efficiency.", "citation_counts": 800}, {"title": "REALM: Retrieval-Augmented Language Model Pre-Training", "text": "REALM pre-trains a language model as a dense retriever and a knowledge-augmented encoder. This approach allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, improving performance on knowledge-intensive tasks. The retrieval component is learned end-to-end as part of the training process.", "citation_counts": 900}, {"title": "RAG vs Parametric Models", "text": "Traditional parametric language models store knowledge in their weights, which can lead to hallucinations and outdated information. Retrieval-augmented models address these issues by accessing external knowledge bases during inference, providing more accurate and up-to-date information while maintaining the flexibility of neural generation.", "citation_counts": 200}]} 